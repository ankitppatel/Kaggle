{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNP Paribas Cardif Insurance Kaggle competition\n",
    "In this Kaggle competition, sponsored by BNP Paribas Cardif, participants were challenged to utilize a set of features, which would typically be available during the early stages of an insurance claim, to predict whether the claim could be accelerated towards the payment phase or if additional investigation would be needed by BNP. The seemingly obvious benefit of speeding up claims processing would be the improved level of service provided to customers of BNP. The risk, however, is paying expedited claims inaccurately may incur losses. The goal was to predict the probability of having a Target outcome of “1”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of completely anonymous data, there was essentially no room for ‘domain expertise’ to be used here (ie. for feature selection or feature engineering), only data wrangling and machine learning tactics would come in handy. A significant portion of the data was found to be missing, which required arbitrarily dropping some features and using substitute values to fill the gaps on others. <br>\n",
    "Since this problem is a Binary classification, one of the first models to come to mind is the commonly used Logistic Regression. We additionally used a range of individual Tree based classifiers, which employed various ‘enhancements’ to their predictive capabilities through the use of bagging, boosting, and other randomization techniques. <br>\n",
    "Ultimately we used a method, popular on Kaggle, called Stacking. Stacking, intuitively speaking, 'learns' from the different 'opinions' provided by a group of models, which may not be very strong individually, but collectively can be more effective in choosing an accurate prediction (kind of a \"people's choice\", without significant group think). From a very basic Logistic Regression model to the much more complex stacked model, a significant improvement in score was made. <br>\n",
    "Thinking about where improvements could’ve been made: During the data cleaning/manipulation process, a lot of data was dropped due to a high number of null values. Perhaps those null values meant something? Other blending techniques, besides Stacking, could’ve been used to see if similar or greater improvements in score can be achieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations and Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Show any plotted graphs within the notebook\n",
    "% matplotlib inline\n",
    "\n",
    "import pandas as pd #Data manipulation library\n",
    "import numpy as np #Numerical analysis and linear algebra library\n",
    "\n",
    "# Matplotlib provides the ability to plot figures and charts in python\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn provides various scientific functionality as well as machine learning models for data analysis\n",
    "import sklearn \n",
    "\n",
    "from sklearn.cross_validation import KFold, StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder #take categorical data and convert them into numerical representations\n",
    "from sklearn.metrics import log_loss #the risk function we will be utilizing to evaluate our model\n",
    "\n",
    "# Import the models we will be using as the base composition of our final predictive ensemble model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "\n",
    "# eXtreme Gradient boosting library. Faster, more efficient, and more effective implementation of\n",
    "# gradient boosting modeling\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# miscellaneous libraries to help keep track of code run time, various math operations etc\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions :\n",
    "**1) cv_score <br>\n",
    "2) do_GridsearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Model: cv_score\n",
    "\n",
    "# Description:\n",
    "# Do a 5 fold cross validation to calculate log_loss of the current model\n",
    "\n",
    "# Input Parameters:\n",
    "# clf - the sklearn model being cross-validated\n",
    "# x - data matrix containing the features used to train models/predict outcomes\n",
    "# y - target variable\n",
    "\n",
    "# Return value:\n",
    "# The average log_loss score achieved from 5 train/test iterations\n",
    "##########################\n",
    "def cv_score(clf, x, y):\n",
    "    nfolds = 5\n",
    "    score = 0\n",
    "    for train, test in KFold(y.size, n_folds = nfolds ):\n",
    "        xTrain = x[train]\n",
    "        yTrain = y[train]\n",
    "        xTest = x[test]\n",
    "        yTest = y[test]\n",
    "        \n",
    "        clf.fit(xTrain, yTrain)\n",
    "        predictions = clf.predict_proba(xTest)\n",
    "        score += log_loss(yTest, predictions)\n",
    "    \n",
    "    return score/nfolds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Model: do_GridsearchCV\n",
    "\n",
    "# Description:\n",
    "# Use cross-validated GridSearch to find best model parameters which optimizes our scoring objective\n",
    "\n",
    "# Input Parameters:\n",
    "# clf - the sklearn model being cross-validated\n",
    "# params - dictionary of parameter names and their respective parameter values to be tested for optimality \n",
    "# scorer - the scoring function that the GridSearch will try to optimize (minimizing log_loss in our current case)\n",
    "# xTrain - data matrix containing the features used to train models/predict outcomes\n",
    "# yTrain - target variable\n",
    "# folds - number of folds to be used during the cross-validation process\n",
    "\n",
    "# Return values:\n",
    "# clf - optimized estimator model\n",
    "# clf.grid_scores_ - the paramater combos tested and their resulting score\n",
    "# clf.best_params_ - the best scoring paramater combo\n",
    "# clf.best_score_ - the best score achieved from the best params\n",
    "##########################\n",
    "\n",
    "def do_GridsearchCV(clf, params, scorer, xTrain, yTrain, folds = 5):\n",
    "    startTime = time.time()\n",
    "    clf = GridSearchCV(clf, param_grid = params, scoring = scorer, cv = folds)\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    training_acc = clf.score(xTrain, yTrain)\n",
    "    print \"Training Accuracy: %0.2f\" % training_acc\n",
    "    print clf.grid_scores_, clf.best_params_, clf.best_score_\n",
    "    print \"\\nTime GridSearchCV took to run: %r minutes\" % ((time.time()-startTime)/60)\n",
    "\n",
    "    return clf, clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data provided is completely anonymous. We have no knowledge of each feature's qualitative relevance, they are simply labeled v1 to v131. We also don't know what a target value of \"1\" means, we just know that we have to predict the probability of the outcome being \"1\" given a set of features.\n",
    "\n",
    "There is a lot of missing data (NaN values) for many of the columns, some of which are missing almost half of their observations. <br> While most of the data is numerical values, we have a number of columns that contain categorical values, that will need to be changed later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in the training set, test set, and sample submission format\n",
    "trainData = pd.read_csv('data/train.csv')\n",
    "testData = pd.read_csv('data/test.csv')\n",
    "sampleSubmission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v122</th>\n",
       "      <th>v123</th>\n",
       "      <th>v124</th>\n",
       "      <th>v125</th>\n",
       "      <th>v126</th>\n",
       "      <th>v127</th>\n",
       "      <th>v128</th>\n",
       "      <th>v129</th>\n",
       "      <th>v130</th>\n",
       "      <th>v131</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>C</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.989780</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>AU</td>\n",
       "      <td>1.804126</td>\n",
       "      <td>3.113719</td>\n",
       "      <td>2.024285</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636365</td>\n",
       "      <td>2.857144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.191265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.301630</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598896</td>\n",
       "      <td>AF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.957825</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943877</td>\n",
       "      <td>5.310079</td>\n",
       "      <td>C</td>\n",
       "      <td>4.410969</td>\n",
       "      <td>5.326159</td>\n",
       "      <td>3.979592</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>0.019645</td>\n",
       "      <td>...</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>2.477596</td>\n",
       "      <td>0.013452</td>\n",
       "      <td>AE</td>\n",
       "      <td>1.773709</td>\n",
       "      <td>3.922193</td>\n",
       "      <td>1.120468</td>\n",
       "      <td>2</td>\n",
       "      <td>0.883118</td>\n",
       "      <td>1.176472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.797415</td>\n",
       "      <td>8.304757</td>\n",
       "      <td>C</td>\n",
       "      <td>4.225930</td>\n",
       "      <td>11.627438</td>\n",
       "      <td>2.097700</td>\n",
       "      <td>1.987549</td>\n",
       "      <td>0.171947</td>\n",
       "      <td>...</td>\n",
       "      <td>7.018256</td>\n",
       "      <td>1.812795</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>CJ</td>\n",
       "      <td>1.415230</td>\n",
       "      <td>2.954381</td>\n",
       "      <td>1.990847</td>\n",
       "      <td>1</td>\n",
       "      <td>1.677108</td>\n",
       "      <td>1.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  target        v1        v2 v3        v4         v5        v6        v7  \\\n",
       "0   3       1  1.335739  8.727474  C  3.921026   7.915266  2.599278  3.176895   \n",
       "1   4       1       NaN       NaN  C       NaN   9.191265       NaN       NaN   \n",
       "2   5       1  0.943877  5.310079  C  4.410969   5.326159  3.979592  3.928571   \n",
       "3   6       1  0.797415  8.304757  C  4.225930  11.627438  2.097700  1.987549   \n",
       "4   8       1       NaN       NaN  C       NaN        NaN       NaN       NaN   \n",
       "\n",
       "         v8    ...         v122      v123      v124  v125      v126      v127  \\\n",
       "0  0.012941    ...     8.000000  1.989780  0.035754    AU  1.804126  3.113719   \n",
       "1  2.301630    ...          NaN       NaN  0.598896    AF       NaN       NaN   \n",
       "2  0.019645    ...     9.333333  2.477596  0.013452    AE  1.773709  3.922193   \n",
       "3  0.171947    ...     7.018256  1.812795  0.002267    CJ  1.415230  2.954381   \n",
       "4       NaN    ...          NaN       NaN       NaN     Z       NaN       NaN   \n",
       "\n",
       "       v128  v129      v130      v131  \n",
       "0  2.024285     0  0.636365  2.857144  \n",
       "1  1.957825     0       NaN       NaN  \n",
       "2  1.120468     2  0.883118  1.176472  \n",
       "3  1.990847     1  1.677108  1.034483  \n",
       "4       NaN     0       NaN       NaN  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a look at the first 5 lines of data in your dataframe\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:  (114321, 133)\n",
      "ID            0\n",
      "target        0\n",
      "v1        49832\n",
      "v2        49796\n",
      "v3         3457\n",
      "v4        49796\n",
      "v5        48624\n",
      "v6        49832\n",
      "v7        49832\n",
      "v8        48619\n",
      "v9        49851\n",
      "v10          84\n",
      "v11       49836\n",
      "v12          86\n",
      "v13       49832\n",
      "v14           4\n",
      "v15       49836\n",
      "v16       49895\n",
      "v17       49796\n",
      "v18       49832\n",
      "v19       49843\n",
      "v20       49840\n",
      "v21         611\n",
      "v22         500\n",
      "v23       50675\n",
      "v24           0\n",
      "v25       48619\n",
      "v26       49832\n",
      "v27       49832\n",
      "v28       49832\n",
      "          ...  \n",
      "v102      51316\n",
      "v103      49832\n",
      "v104      49832\n",
      "v105      48658\n",
      "v106      49796\n",
      "v107          3\n",
      "v108      48624\n",
      "v109      48624\n",
      "v110          0\n",
      "v111      49832\n",
      "v112        382\n",
      "v113      55304\n",
      "v114         30\n",
      "v115      49895\n",
      "v116      49836\n",
      "v117      48624\n",
      "v118      49843\n",
      "v119      50680\n",
      "v120      49836\n",
      "v121      49840\n",
      "v122      49851\n",
      "v123      50678\n",
      "v124      48619\n",
      "v125         77\n",
      "v126      49832\n",
      "v127      49832\n",
      "v128      48624\n",
      "v129          0\n",
      "v130      49843\n",
      "v131      49895\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# See how many rows of observations and columns of features do we have\n",
    "print \"Shape of training data: \",trainData.shape\n",
    "\n",
    "# Tally the number missing data points in each column\n",
    "print (trainData.isnull()*1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID          int64\n",
       "target      int64\n",
       "v1        float64\n",
       "v2        float64\n",
       "v3         object\n",
       "v4        float64\n",
       "v5        float64\n",
       "v6        float64\n",
       "v7        float64\n",
       "v8        float64\n",
       "v9        float64\n",
       "v10       float64\n",
       "v11       float64\n",
       "v12       float64\n",
       "v13       float64\n",
       "v14       float64\n",
       "v15       float64\n",
       "v16       float64\n",
       "v17       float64\n",
       "v18       float64\n",
       "v19       float64\n",
       "v20       float64\n",
       "v21       float64\n",
       "v22        object\n",
       "v23       float64\n",
       "v24        object\n",
       "v25       float64\n",
       "v26       float64\n",
       "v27       float64\n",
       "v28       float64\n",
       "           ...   \n",
       "v102      float64\n",
       "v103      float64\n",
       "v104      float64\n",
       "v105      float64\n",
       "v106      float64\n",
       "v107       object\n",
       "v108      float64\n",
       "v109      float64\n",
       "v110       object\n",
       "v111      float64\n",
       "v112       object\n",
       "v113       object\n",
       "v114      float64\n",
       "v115      float64\n",
       "v116      float64\n",
       "v117      float64\n",
       "v118      float64\n",
       "v119      float64\n",
       "v120      float64\n",
       "v121      float64\n",
       "v122      float64\n",
       "v123      float64\n",
       "v124      float64\n",
       "v125       object\n",
       "v126      float64\n",
       "v127      float64\n",
       "v128      float64\n",
       "v129        int64\n",
       "v130      float64\n",
       "v131      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What types of data are we working with (ie. Numerical - integers and/or floats, Categorical - strings and/or objects)\n",
    "trainData.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are almost three times as many \"1\" values as there are \"0\" values, expecting to having a 76% chance of getting a \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Mean: 0.76, Target std: 0.43 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10a401750>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEZCAYAAACq1zMoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1097519d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Target Mean: %.2f, Target std: %.2f \\n\" % (trainData.target.mean(), trainData.target.std())\n",
    "\n",
    "plt.hist(trainData.target, label = 'x', alpha = .5)\n",
    "plt.title(\"Distribution of Target Values\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all of the train and test data so that we can uniformly apply data manipulation techniques to everything.\n",
    "In order to clean up the data, an assumption was made that columns with a large number of null values probably won't contribute much information to model training. The column-drop threshold was set at >11,000 (~10%) null values of training data. \n",
    "For the columns that have categorical data, use **LabelEncoder** function from **SKLearn** to enumerate each unique category (our combined data allows us to maintain the same encoding scheme for both train/test sets).\n",
    "For the values that are still null, apply a generic numerical value (\"-1\") to fill in the blanks.\n",
    "Finally, we split the cleaned data back into their original train and test groups, making sure that we didn't lose any rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (114321, 133), Test shape (114393, 132), Combined shape (228714, 133)\n"
     ]
    }
   ],
   "source": [
    "#Combine train and test data\n",
    "\n",
    "combinedData = pd.concat([trainData, testData], axis = 0,ignore_index = True)\n",
    "print \"Train shape: %r, Test shape %r, Combined shape %r\" % (trainData.shape, testData.shape, combinedData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'target', 'v3', 'v10', 'v12', 'v14', 'v21', 'v22', 'v24', 'v31', 'v34', 'v38', 'v40', 'v47', 'v50', 'v52', 'v56', 'v62', 'v66', 'v71', 'v72', 'v74', 'v75', 'v79', 'v91', 'v107', 'v110', 'v112', 'v114', 'v125', 'v129']\n"
     ]
    }
   ],
   "source": [
    "# Drop unwanted columns that have >11000 null training values\n",
    "\n",
    "usableColumns = [column for column in trainData.columns if (trainData[column].isnull()).sum() <11000]\n",
    "print usableColumns\n",
    "cleanCombinedData = combinedData[usableColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankitpatel/anaconda/lib/python2.7/site-packages/pandas/core/generic.py:2862: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/Users/ankitpatel/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column: v3, before: object After int64\n",
      "column: v22, before: object After int64\n",
      "column: v24, before: object After int64\n",
      "column: v31, before: object After int64\n",
      "column: v47, before: object After int64\n",
      "column: v52, before: object After int64\n",
      "column: v56, before: object After int64\n",
      "column: v66, before: object After int64\n",
      "column: v71, before: object After int64\n",
      "column: v74, before: object After int64\n",
      "column: v75, before: object After int64\n",
      "column: v79, before: object After int64\n",
      "column: v91, before: object After int64\n",
      "column: v107, before: object After int64\n",
      "column: v110, before: object After int64\n",
      "column: v112, before: object After int64\n",
      "column: v125, before: object After int64\n"
     ]
    }
   ],
   "source": [
    "# Encode all of the categorical values into numerical values\n",
    "\n",
    "for column in cleanCombinedData.columns:\n",
    "    if (cleanCombinedData[column].dtype != 'int64') and (cleanCombinedData[column].dtype != 'float64'):\n",
    "        print \"column: %s, before: %s\" % (column, cleanCombinedData[column].dtype),\n",
    " \n",
    "        #assume empty spaces in this column mean same thing and dummy-fill them before encoding\n",
    "        cleanCombinedData[column].fillna('empty', inplace = True) \n",
    "        \n",
    "        encoder = LabelEncoder()\n",
    "        cleanCombinedData[column] = encoder.fit_transform(cleanCombinedData[column])\n",
    "        print \"After %s\" % cleanCombinedData[column].dtype #this helps us keep track of the data type before and after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankitpatel/anaconda/lib/python2.7/site-packages/pandas/core/frame.py:2705: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# for the remaining na/NaN values, fill them with -1\n",
    "cleanCombinedData.fillna(-1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig Train: (114321, 133), New Train: (114321, 31). Orig Test: (114393, 132), New Test: (114393, 31)\n"
     ]
    }
   ],
   "source": [
    "# split the combined data back out again into the original Train and Test splits\n",
    "cleanTrainData = cleanCombinedData[:trainData.shape[0]]\n",
    "cleanTestData = cleanCombinedData[trainData.shape[0]:]\n",
    "\n",
    "#check to make sure we didn't lose any rows of Data, and see how many columns we dropped\n",
    "print \"Orig Train: %r, New Train: %r. Orig Test: %r, New Test: %r\" % (trainData.shape, cleanTrainData.shape,\n",
    "                                                                     testData.shape, cleanTestData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this problem is a Binary classification, one of the first models to come to mind is the commonly used Logistic Regression. <br>\n",
    "**Logistic regression** - relatively simple and allows us to obey the contraint of having outcomes 0/1, it also has low variance and so is less prone to over-fitting. \n",
    "\n",
    "We also consider: <br>\n",
    "**Decision Trees** - A Decision tree can be scaled up to be very complex, and can be applied to situations where there's not just one underlying decision boundary, but many. Given our data set of many features, this seems like something that can be useful to us. However, Decision Trees can be notoriously over-fit to the train data, rendering them ineffective for generalization. \n",
    "\n",
    "**Random Forest Classifier** - Allows us to utilize the multiple boundary decision-making process of a DT but reduce variance. Variance reduction is done through using many DTs (Ensemble method), each tree using a random subset of training points with replacement (Bagging method), and each tree uses randomly selected features upon which to base our decision boundaries on. \n",
    "\n",
    "**Extremely Randomized Trees (Extra Trees)** - Same as a Random Forest classifier, except that for each feature used in  the decision boundaries, the split value is randomly chosen from that feature's empirical range.\n",
    "\n",
    "**Gradient Boosting**. - Going one step further, Gradient boosting combines a series of weak learners (for example, the various tree based learners described above), and with each iterative step adds another learner to correct the errors generated from the prior learners. \n",
    "\n",
    "While most of the models we'll implement are part of the SKLearn package, we will also use the **XGBoost** package. XGBoost (Extreme Gradient Boosting) is an optimized, efficient and very accurate implementation of the Gradient Boosting model that has gained notoriety as being a model of choice in many winning Kaggle competitions. \n",
    "\n",
    "Our objective was to minimize the **logarithmic loss function**:\n",
    "$$ log loss = -\\frac{1}{N}\\sum_{i=1}^N {(y_i\\log(p_i) + (1 - y_i)\\log(1 - p_i))} $$\n",
    "where N is the number of observations, \\\\(log\\\\) is the natural logarithm, $y_i$ is the binary target, and $p_i$ is the predicted probability that $y_i$ equals 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying out a few of the models we're considering and see how they fair, using default parameter settings. <br>\n",
    "We can see that Random Forest was the poorest performing model, while XGBoost was the best performing one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False):, score: 0.49544555530872658\n",
      "model XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1):, score: 0.4743854565921059\n",
      "model RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False):, score: 1.002004225668689\n"
     ]
    }
   ],
   "source": [
    "Log = LogisticRegression()\n",
    "XGB = XGBClassifier() \n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "models = [Log, XGB, RFC]\n",
    "\n",
    "xData = cleanTrainData.drop(['ID', \"target\"], axis = 1).values\n",
    "yData = cleanTrainData['target'].values\n",
    "for model in models:\n",
    "    print \"model %s:, score: %r\" %(model, cv_score(model,xData, yData ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stay with the XGBoost model for the moment. We will use the GridSearch function from SKLearn to test a number of different parameter values to find the best combination that optimizes our scoring function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: -0.43\n",
      "[mean: -0.46871, std: 0.00112, params: {'max_depth': 4}, mean: -0.46818, std: 0.00126, params: {'max_depth': 5}, mean: -0.46914, std: 0.00110, params: {'max_depth': 6}] {'max_depth': 5} -0.468184649306\n",
      "\n",
      "Time GridSearchCV took to run: 18.9333456158638 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankitpatel/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:418: ChangedBehaviorWarning: The long-standing behavior to use the estimator's score function in GridSearchCV.score has changed. The scoring parameter is now used.\n",
      "  ChangedBehaviorWarning)\n"
     ]
    }
   ],
   "source": [
    "# params to be fine tuned on XGB, max_depth, n_estimators, reg_alpha, reg_lambda\n",
    "#XGB = XGBClassifier(max_depth = 5, n_estimators = 450, reg_alpha = 0.1, reg_lambda = 5, learning_rate = .1, \n",
    "#                    subsample = 1) \n",
    "\n",
    "# set up a basic XGB model, with some params that i've already tested before\n",
    "XGB = XGBClassifier(n_estimators = 450, learning_rate = .1, subsample = 1, reg_alpha = 0.1, reg_lambda = 5 ) \n",
    "\n",
    "params = {'max_depth': [4,5,6]}\n",
    "\n",
    "xData = cleanTrainData.drop(['ID', 'target'], axis = 1).values\n",
    "yData = cleanTrainData['target'].values\n",
    "XGBgridSearch, gridScores, bestParams, bestScore = do_GridsearchCV(XGB, params, \"log_loss\", xData, yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5}\n",
      "-0.468184649306\n"
     ]
    }
   ],
   "source": [
    "# print out what my best parameter value combination is as well as the resulting score\n",
    "print bestParams\n",
    "print bestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PredictedProb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.307977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.957670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.884944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.670062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.766797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  PredictedProb\n",
       "0   0       0.307977\n",
       "1   1       0.957670\n",
       "2   2       0.884944\n",
       "3   7       0.670062\n",
       "4  10       0.766797"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the best parameters found, re-train the XGB model and predict outcomes on test set\n",
    "XGB = XGBClassifier(max_depth = 5, n_estimators = 450, reg_alpha = 0.1, reg_lambda = 5, learning_rate = .1, \n",
    "                    subsample = 1)\n",
    "\n",
    "xData = cleanTrainData.drop(['ID', 'target'], axis = 1)\n",
    "yData = cleanTrainData['target']\n",
    "xTest = cleanTestData.drop(['ID', 'target'], axis = 1)\n",
    "XGB.fit(xData, yData)\n",
    "\n",
    "yPred = XGB.predict_proba(xTest)\n",
    "\n",
    "# take the data from the second column, which is the predicted probability of a \"1\"\n",
    "sampleSubmission.PredictedProb = yPred[:,1]\n",
    "# make sure nothing looks weird\n",
    "sampleSubmission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write the predictions to a csv file that can be uploaded to Kaggle\n",
    "# file name has current data and time appended to it, so different predictions can be distinguished \n",
    "\n",
    "writePath = 'data/XGB_prob_submission_%s.csv' % (datetime.datetime.now())\n",
    "#sampleSubmission.to_csv(writePath, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the methods commonly seen in Kaggle competitions is the use of Ensembling and/or Stacking. <br>\n",
    "Ensembling tends to describe the process of training multiple models, who's individual predictions are weighted in some fashion, subsequently producing one collective prediction. <br>\n",
    "Stacking, on the other hand, trains multiple models, makes predictions using those models, and then trains *another* model on those predictions, ultimately using this combiner model to make a final set of predictions. <br>\n",
    "The intuition behind using these approaches is that we can 'learn' from the different 'opinions' provided by a group of models, which may not be very strong individually, but collectively can be more effective in choosing an accurate prediction (kind of a \"people's choice\", without significant group think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for blending.\n",
      "0 LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "1 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "2 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "3 ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "4 ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "5 GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "6 XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=200, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0.1, reg_lambda=5,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Blending.\n",
      "***Score***  0.459985847321\n",
      "Saving to file.\n",
      "Total time taken: 30.82372789780299 minutes\n"
     ]
    }
   ],
   "source": [
    "#Try a model blending approach\n",
    "#Much of this code sourced from: https://github.com/emanuele/kaggle_pbr/blob/master/blend.py\n",
    "\n",
    "X = cleanTrainData.drop(['ID', 'target'], axis = 1).values\n",
    "y = cleanTrainData['target'].values\n",
    "X_submission = cleanTestData.drop(['ID', 'target'], axis = 1)\n",
    "\n",
    "n_folds = 10  #use 10 folds in our cross-validation model building below\n",
    "\n",
    "skf = list(StratifiedKFold(y, n_folds)) # stratifiedKFold provides train/test sets for n_fold iterations, \n",
    "                                        # while maintaining the class proportions of the original target set\n",
    "\n",
    "# our set of base learners (models)\n",
    "clfs = [LogisticRegression(),\n",
    "            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "            GradientBoostingClassifier(learning_rate=0.1, n_estimators=200),\n",
    "            XGBClassifier(n_estimators = 200, reg_alpha = 0.1, reg_lambda = 5, learning_rate = .1)]\n",
    "\n",
    "\n",
    "print \"Creating train and test sets for blending.\"\n",
    "start_time = time.time()\n",
    "#######################################\n",
    "# dataset_blend_train - matrix of (# training samples, # of clfs). It collects all of the predictions made on  \n",
    "#                       'held out' chunks of test sets during the CV process, for each model in our list\n",
    "# dataset_blend_test - matrix of (# of test samples, # of clfs). Collects the average of 10 folds of predictions\n",
    "#                      made on the actual test data; for each model in our list.\n",
    "#######################################\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))\n",
    "    \n",
    "for j, clf in enumerate(clfs):\n",
    "    print j, clf  #print classifier number and model info\n",
    "    dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))  #matrix to hold our predictions on the actual\n",
    "                                                                        #final test set for each fold 'i'\n",
    "    #for each fold 'i', do work with the train/test set created\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print \"Fold\", i\n",
    "        X_train = X[train]\n",
    "        y_train = y[train]\n",
    "        X_test = X[test]\n",
    "        y_test = y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict_proba(X_test)[:,1]\n",
    "        dataset_blend_train[test, j] = y_submission  #predictions made on our skf test set are added to the train matrix\n",
    "                                                     #in their respective test rows assigned by skf\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:,1] #predictions made on our actual test data\n",
    "                                                                          #assigned to matrix for each fold  \n",
    "    dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)  #in the overall test matrix, assign the average\n",
    "                                                            #prediction of all i folds on the actual test data\n",
    "                                                            #for each classifier 'j' in our list\n",
    "\n",
    "print\n",
    "print \"Blending.\"\n",
    "\n",
    "# our final \"combiner\" model will be XGB Classifier\n",
    "clf = XGBClassifier(max_depth = 5, n_estimators = 50, reg_alpha = 0.1, reg_lambda = 5, learning_rate = .1, \n",
    "                    subsample = 1)\n",
    "\n",
    "# \"train on predictions, and make predictions on predictions\"\n",
    "clf.fit(dataset_blend_train, y)  #train combiner model on the predictions made on the training data by each base model\n",
    "y_submission = clf.predict_proba(dataset_blend_test)[:,1] #predict outcome given the averaged test predictions of each\n",
    "                                                          #base model\n",
    "\n",
    "#do an independent, local cross-validation to see what the final results might score \n",
    "score = cv_score(clf,dataset_blend_train, y )\n",
    "print \"***Score*** \", score\n",
    "\n",
    "# if i've beaten my benchmark score, then I'm going to save my current results down to csv for Kaggle submission\n",
    "benchmark_score = 0.46\n",
    "if score < benchmark_score:\n",
    "    sampleSubmission.PredictedProb = y_submission\n",
    "    print \"Saving to file.\"\n",
    "    writePath = 'data/Blended_prob1_submission_%s.csv' % (datetime.datetime.now())\n",
    "    sampleSubmission.to_csv(writePath, index = False)\n",
    "\n",
    "    \n",
    "print \"Total time taken: %r minutes\" % ((time.time()-start_time)/60) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a quick look at the distribution of the final set of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x109897e90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEZCAYAAACq1zMoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBar1wCCQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10987c850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_submission, label = 'x', alpha = .5)\n",
    "plt.title(\"Distribution of Final Predictions\")\n",
    "plt.xlabel(\"Probability of 1\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some final fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check to see if the XGB Classifier used above is the best combiner model, we test a few other classification models using local cross-validation to see how they would compare. <br>\n",
    "While Logistic Regression came close, XGB Classifier was still quite a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=50, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0.1, reg_lambda=5,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1):, score: 0.45998584732110548\n",
      "model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False):, score: 0.46537326486119418\n",
      "model RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False):, score: 1.1168342203071835\n",
      "model ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False):, score: 1.1191647376733578\n"
     ]
    }
   ],
   "source": [
    "XGB = XGBClassifier(max_depth = 5, n_estimators = 50, reg_alpha = 0.1, reg_lambda = 5, learning_rate = .1, \n",
    "                    subsample = 1)\n",
    "LOG = LogisticRegression()\n",
    "RFC = RandomForestClassifier()\n",
    "ETC = ExtraTreesClassifier()\n",
    "\n",
    "models = [XGB, LOG, RFC, ETC]\n",
    "\n",
    "xData = dataset_blend_train\n",
    "yData = y\n",
    "for model in models:\n",
    "    print \"model %s:, score: %r\" %(model, cv_score(model,xData, yData ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an effort to squeeze out every last bit of performance from our combiner model, let's do a GridSearchCV to find the best parameters to use when making our final predictions for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: -0.45\n",
      "[mean: -0.46403, std: 0.00135, params: {'learning_rate': 0.05}, mean: -0.45991, std: 0.00171, params: {'learning_rate': 0.1}, mean: -0.46015, std: 0.00178, params: {'learning_rate': 0.2}] {'learning_rate': 0.1} -0.459912645895\n",
      "\n",
      "Time GridSearchCV took to run: 0.7771179477373759 minutes\n"
     ]
    }
   ],
   "source": [
    "XGB = XGBClassifier(max_depth = 5, n_estimators = 50, reg_alpha = 0.1, reg_lambda = 5, learning_rate = .1, \n",
    "                    subsample = 1)\n",
    "params = {'learning_rate' :[.05, .1, .2] }\n",
    "\n",
    "xData = dataset_blend_train\n",
    "yData = y\n",
    "XGBgridSearch, gridScores, bestParams, bestScore = do_GridsearchCV(XGB, params, \"log_loss\", xData, yData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestParams:  {'learning_rate': 0.1}\n",
      "bestScore: -0.459912645895\n"
     ]
    }
   ],
   "source": [
    "print \"bestParams: \", bestParams\n",
    "print \"bestScore:\", bestScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a very basic Logistic Regression model to the much more complex stacked model, a significant improvement in score was made. Where else could improvements have been made? <br>\n",
    "During the data cleaning/manipulation process, a lot of data was dropped due to a high number of null values. Perhaps those null values meant something? We could've kept them, while filling them with a some dummy value. We could've also created another column (feature engineering) that would indicate a 0/1 representation of whether or not data existed for that particular feature. <br>\n",
    "We could've also tried a different variation of model blending techniques, increased the amount of base learners that were blended, or even tweaking the base learners that went into the blend. <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
