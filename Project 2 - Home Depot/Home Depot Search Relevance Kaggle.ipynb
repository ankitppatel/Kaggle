{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Depot Search Relevance\n",
    "In this Kaggle competition, sponsored by Home Depot, participants were challenged to utilize a set of real customer search terms to predict how relevant they were to the particular product that Home Depot's website returned as a result. The relevance number would range from 1 (not relevant at all) to 3 (very relevant). Each search-term/product pair was evaluated by at least three real human people and the average of those values is what we will need to predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations and Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Show any plotted graphs within the notebook\n",
    "% matplotlib inline\n",
    "\n",
    "import sys\n",
    "import re, collections\n",
    "\n",
    "import pandas as pd #Data manipulation library\n",
    "import numpy as np #Numerical analysis and linear algebra library\n",
    "\n",
    "# Matplotlib provides the ability to plot figures and charts in python\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn provides various scientific functionality as well as machine learning models for data analysis\n",
    "import sklearn \n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, accuracy_score\n",
    "from sklearn.cross_validation import KFold, StratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #to create a bag of words out of our text\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Import the models we will be using as the base composition of our final predictive ensemble model\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "\n",
    "# eXtreme Gradient boosting library. Faster, more efficient, and more effective implementation of\n",
    "# gradient boosting modeling\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import string\n",
    "from string import maketrans\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import HTMLParser\n",
    "html_parser = HTMLParser.HTMLParser()\n",
    "\n",
    "\n",
    "# miscellaneous libraries to help keep track of code run time, various math operations etc\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions :\n",
    "** 1) RMSE (Root Mean Squared Error)<br>\n",
    "2) cv_score <br>\n",
    "3) do_GridsearchCV <br>\n",
    "4) replaceAll <br>\n",
    "5) addSpaces <br>\n",
    "6) spellCheck <br>\n",
    "7) cleanText <br>\n",
    "8) clean_my_predictions (currently not used) <br>\n",
    "9) compareWordHits\n",
    "**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the Root Mean Squared Error of a given prediction\n",
    "# Use the \"make_scorer\" function from SKLearn to build your own \"root\" MSE scorer to be used in other functions\n",
    "def RMSE(yTrue, yPred):\n",
    "    return math.sqrt(mean_squared_error(yTrue,yPred))\n",
    "\n",
    "RootMSE = make_scorer(RMSE, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# function: cv_score\n",
    "\n",
    "# Description:\n",
    "# Do a 5 fold cross validation to calculate RMSE of the current model\n",
    "\n",
    "# Input Parameters:\n",
    "# clf - the sklearn model being cross-validated\n",
    "# x - data matrix containing the features used to train models/predict outcomes\n",
    "# y - target variable\n",
    "\n",
    "# Return value:\n",
    "# The average log_loss score achieved from 5 train/test iterations\n",
    "##########################\n",
    "def cv_score(clf, x, y):\n",
    "    nfolds = 5\n",
    "    score = 0\n",
    "    for train, test in KFold(y.size, n_folds = nfolds ):\n",
    "        xTrain = x[train]\n",
    "        yTrain = y[train]\n",
    "        xTest = x[test]\n",
    "        yTest = y[test]\n",
    "        \n",
    "        clf.fit(xTrain, yTrain)\n",
    "        predictions = clf.predict(xTest)\n",
    "        score += math.sqrt(mean_squared_error(yTest, predictions))\n",
    "    \n",
    "    return score/nfolds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# function: do_GridsearchCV\n",
    "\n",
    "# Description:\n",
    "# Use cross-validated GridSearch to find best model parameters which optimizes our scoring objective\n",
    "\n",
    "# Input Parameters:\n",
    "# clf - the sklearn model being cross-validated\n",
    "# params - dictionary of parameter names and their respective parameter values to be tested for optimality \n",
    "# scorer - the scoring function that the GridSearch will try to optimize (minimizing log_loss in our current case)\n",
    "# xTrain - data matrix containing the features used to train models/predict outcomes\n",
    "# yTrain - target variable\n",
    "# folds - number of folds to be used during the cross-validation process\n",
    "\n",
    "# Return values:\n",
    "# clf - optimized estimator model\n",
    "# clf.grid_scores_ - the paramater combos tested and their resulting score\n",
    "# clf.best_params_ - the best scoring paramater combo\n",
    "# clf.best_score_ - the best score achieved from the best params\n",
    "##########################\n",
    "\n",
    "def do_GridsearchCV(clf, params, scorer, xTrain, yTrain, folds = 5):\n",
    "    startTime = time.time()\n",
    "    clf = GridSearchCV(clf, param_grid = params, scoring = scorer, cv = folds)\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    training_acc = clf.score(xTrain, yTrain)\n",
    "    print \"Training Accuracy: %0.2f\" % training_acc\n",
    "    print clf.grid_scores_, clf.best_params_, clf.best_score_\n",
    "    print \"\\nTime GridSearchCV took to run: %r minutes\" % ((time.time()-startTime)/60)\n",
    "\n",
    "    return clf, clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# function: replaceAll\n",
    "\n",
    "# Description:\n",
    "# Replace all of the \"search\" terms with the \"replace\" values in a given sentence\n",
    "\n",
    "# Input Parameters:\n",
    "# sentence - the sentence upon which work will be done\n",
    "# search - the term to be replaced\n",
    "# replace - the new term to replace the original one\n",
    "\n",
    "# Return values:\n",
    "# sentence - return the amended sentence\n",
    "##########################\n",
    "def replaceAll(sentence, search, replace):\n",
    "    for old, new in zip(search, replace):\n",
    "        sentence = sentence.replace(old, new)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# the product descriptions provided by HomeDepot are not properly spaced, \n",
    "# therefore, for each bullet point in a given product's attributes, add a space before it\n",
    "############################\n",
    "def addSpaces(row):\n",
    "    spaceWords = []\n",
    "    #for each bullet available for the currently given product...\n",
    "    for bullet in bulletPoints[bulletPoints.product_uid == row.product_uid].value:\n",
    "        spaceWords.append(bullet.split(\" \")[0])   #take the first word in each bullet\n",
    "    \n",
    "    #append a space to the beginning of each word in 'spaceWords'\n",
    "    newWords = [\"\".join(pair) for pair in (zip(len(spaceWords)*\" \", spaceWords ))]\n",
    "    return replaceAll(row.product_description,spaceWords, newWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# using a generic spell-check algorithm found online, take a sentence and correct as many mistakes as possible\n",
    "##################\n",
    "def spellCheck(sentence):\n",
    "    for word in sentence.split():\n",
    "        newWord = correct(word)\n",
    "        sentence = sentence.replace(word, newWord)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# take a given sentence and \"clean\" each word based on the following steps\n",
    "##################\n",
    "def cleanText(sentence):\n",
    "\n",
    "    # 1) remove HTML tags\n",
    "    try:\n",
    "        sentence = html_parser.unescape(sentence).encode('ascii', 'ignore')\n",
    "    except Exception:\n",
    "        sys.exc_clear()\n",
    "\n",
    "    # 2) make everything lower case\n",
    "    sentence = sentence.lower()\n",
    "    # 3) remove weblinks and URLs\n",
    "    for word in sentence.split():\n",
    "        if (\"www\" in word) or (\"http\" in word):\n",
    "            sentence = sentence.replace(word, \"\")\n",
    "\n",
    "    # 4) remove English language contractions \n",
    "    bigContractions = {\"don't\":\"do not\", \"won't\":\"will not\"}\n",
    "    sentence = replaceAll(sentence, bigContractions.keys(), bigContractions.values())\n",
    "    smallContractions = {\"'em\": \" them\", \"'d\" : \" would\", \"'ll\": \" will\",\"'m\": \" am\", \"'n\":\"ing\", \n",
    "                         \"n't\":\" not\",  \"'ve\":\" have\" }\n",
    "    sentence = replaceAll(sentence, smallContractions.keys(), smallContractions.values())\n",
    "    \n",
    "    # 5) get rid of unwanted punctations, keeping (\"-\" and \"/\") because they are often used in sizing\n",
    "    exclude = string.punctuation\n",
    "    exclude = exclude.replace('-/', '')\n",
    "    #trantab = maketrans(exclude, \"\"*len(exclude))\n",
    "    sentence = sentence.translate(None, exclude)\n",
    "    \n",
    "    # 6) get rid of english stop words as provided by NLTK\n",
    "    stop = stopwords.words('english')\n",
    "    sentence = replaceAll(sentence, stop, \"\"*len(stop) )\n",
    "    \n",
    "    # 7) Stem words using Porter Stemmer from NLTK\n",
    "    try:\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmedWords = [stemmer.stem(word) for word in sentence.split()]\n",
    "        sentence = \" \".join(stemmedWords).encode('ascii', 'ignore')\n",
    "    except Exception:\n",
    "        sys.exc_clear()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# Assuming a set number of human evaluators who would've provided a score, create a set of possible\n",
    "# averages that your predictions should be. Then you can alter your prediction to the nearest possibility\n",
    "#################\n",
    "def clean_my_prediction(predictions):\n",
    "    # Create a list of possible outcomes assuming three to five people providing training scores\n",
    "\n",
    "    #three = np.unique([round(np.asarray(i).mean(),2) for i in itertools.combinations_with_replacement(range(4),3)])\n",
    "    #four = np.unique([round(np.asarray(i).mean(),2) for i in itertools.combinations_with_replacement(range(4),4)])\n",
    "    #five = np.unique([round(np.asarray(i).mean(),2) for i in itertools.combinations_with_replacement(range(4),5)])\n",
    "    #train = trainData.relevance.unique()\n",
    "    #possibleScores = np.unique(np.concatenate((three,train), axis = 0))\n",
    "    #possibleScores = three\n",
    "\n",
    "    returnValues = []\n",
    "    for i in predictions:\n",
    "        returnValues.append(min(possibleScores,key = lambda x: abs(x-i)))\n",
    "    \n",
    "    return returnValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Find out the proportion of product title words are in the search terms, as well as proportion of the search terms\n",
    "# are in the product title\n",
    "#################\n",
    "def compareWordHits(row):\n",
    "    prodWordMatch = 0.0\n",
    "    searchWordMatch = 0.0\n",
    "    for prodWord in row.product_title.split(\" \"):\n",
    "        for searchWord in row.search_term.split(\" \"):\n",
    "            if (float(distance(prodWord,searchWord))/max(len(prodWord),len(searchWord))) < 0.20:\n",
    "                prodWordMatch += 1.0\n",
    "                searchWordMatch += 1.0\n",
    "    return (prodWordMatch/len(row.product_title.split(\" \"))) , (searchWordMatch/len(row.search_term.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're provided with three files that are going to be most useful to us: <br>\n",
    "1) **Train** - has product-id, official product title, search terms from real customers, and the assigned relevance score <br> \n",
    "2) **Product_Description** - a detailed description for each product-id in paragraph form <br>\n",
    "3) **Attributes** - a series of bullet points for each product-id, which provide a list of product details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read all of the relevant files\n",
    "trainData = pd.read_csv(\"data/train.csv\")\n",
    "testData = pd.read_csv(\"data/test.csv\")\n",
    "prodDesc = pd.read_csv(\"data/product_descriptions.csv\")\n",
    "\n",
    "attributes = pd.read_csv(\"data/attributes.csv\").dropna()\n",
    "\n",
    "sampleSubmission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "cleanTrain = pd.read_csv('data/trainDataCosine.csv')\n",
    "cleanTest = pd.read_csv('data/testDFCosineSimilarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>l bracket</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>100002</td>\n",
       "      <td>BEHR Premium Textured DeckOver 1-gal. #SC-141 ...</td>\n",
       "      <td>deck over</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>100005</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>rain shower head</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>100005</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>shower only faucet</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                                      product_title  \\\n",
       "0   2       100001                  Simpson Strong-Tie 12-Gauge Angle   \n",
       "1   3       100001                  Simpson Strong-Tie 12-Gauge Angle   \n",
       "2   9       100002  BEHR Premium Textured DeckOver 1-gal. #SC-141 ...   \n",
       "3  16       100005  Delta Vero 1-Handle Shower Only Faucet Trim Ki...   \n",
       "4  17       100005  Delta Vero 1-Handle Shower Only Faucet Trim Ki...   \n",
       "\n",
       "          search_term  relevance  \n",
       "0       angle bracket       3.00  \n",
       "1           l bracket       2.50  \n",
       "2           deck over       3.00  \n",
       "3    rain shower head       2.33  \n",
       "4  shower only faucet       2.67  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>BEHR Premium Textured DECKOVER is an innovativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003</td>\n",
       "      <td>Classic architecture meets contemporary design...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004</td>\n",
       "      <td>The Grape Solar 265-Watt Polycrystalline PV So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100005</td>\n",
       "      <td>Update your bathroom with the Delta Vero Singl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid                                product_description\n",
       "0       100001  Not only do angles make joints stronger, they ...\n",
       "1       100002  BEHR Premium Textured DECKOVER is an innovativ...\n",
       "2       100003  Classic architecture meets contemporary design...\n",
       "3       100004  The Grape Solar 265-Watt Polycrystalline PV So...\n",
       "4       100005  Update your bathroom with the Delta Vero Singl..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prodDesc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>Bullet01</td>\n",
       "      <td>Versatile connector for various 90° connection...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>Bullet02</td>\n",
       "      <td>Stronger than angled nailing or screw fastenin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100001</td>\n",
       "      <td>Bullet03</td>\n",
       "      <td>Help ensure joints are consistently straight a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100001</td>\n",
       "      <td>Bullet04</td>\n",
       "      <td>Dimensions: 3 in. x 3 in. x 1-1/2 in.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100001</td>\n",
       "      <td>Bullet05</td>\n",
       "      <td>Made from 12-Gauge steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100001</td>\n",
       "      <td>Bullet06</td>\n",
       "      <td>Galvanized for extra corrosion resistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100001</td>\n",
       "      <td>Bullet07</td>\n",
       "      <td>Install with 10d common nails or #9 x 1-1/2 in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100001</td>\n",
       "      <td>Gauge</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100001</td>\n",
       "      <td>Material</td>\n",
       "      <td>Galvanized Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100001</td>\n",
       "      <td>MFG Brand Name</td>\n",
       "      <td>Simpson Strong-Tie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid            name  \\\n",
       "0       100001        Bullet01   \n",
       "1       100001        Bullet02   \n",
       "2       100001        Bullet03   \n",
       "3       100001        Bullet04   \n",
       "4       100001        Bullet05   \n",
       "5       100001        Bullet06   \n",
       "6       100001        Bullet07   \n",
       "7       100001           Gauge   \n",
       "8       100001        Material   \n",
       "9       100001  MFG Brand Name   \n",
       "\n",
       "                                               value  \n",
       "0  Versatile connector for various 90° connection...  \n",
       "1  Stronger than angled nailing or screw fastenin...  \n",
       "2  Help ensure joints are consistently straight a...  \n",
       "3              Dimensions: 3 in. x 3 in. x 1-1/2 in.  \n",
       "4                           Made from 12-Gauge steel  \n",
       "5          Galvanized for extra corrosion resistance  \n",
       "6  Install with 10d common nails or #9 x 1-1/2 in...  \n",
       "7                                                 12  \n",
       "8                                   Galvanized Steel  \n",
       "9                                 Simpson Strong-Tie  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:  (74067, 5)\n",
      "id               0\n",
      "product_uid      0\n",
      "product_title    0\n",
      "search_term      0\n",
      "relevance        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print \"Shape of training data: \",trainData.shape\n",
    "print (trainData.isnull()*1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 int64\n",
       "product_uid        int64\n",
       "product_title     object\n",
       "search_term       object\n",
       "relevance        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Mean: 2.38, Target std: 0.53 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x13169f3d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEZCAYAAACq1zMoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE4wkqRcmGElSL0wwkqRemGAkSb34/6Wqqe8kJRhcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x132ad5410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Target Mean: %.2f, Target std: %.2f \\n\" % (trainData.relevance.mean(), trainData.relevance.std())\n",
    "\n",
    "plt.hist(trainData['relevance'], align = 'mid')\n",
    "plt.title(\"Distribution of Target Values\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a number of sources of descriptive data that relate to each product. However, various grammatical and formatting errors can significantly weaken the effectiveness of this data. <br>\n",
    "From our exploratory exercise above, we notice **Product Descriptions** are missing spaces that combine two sentences as one. It appears that each of those \"merged\" sentences are the bullet points from the **Attributes**, so we search for the beginning and end of each of these bullet points in the product description and insert a space between them. We also extract the \"brand name\" attribute so that we can add it into a \"combined product description\" we'll create later. <br>\n",
    "***You can skip the Data Manipulation section if you have both of the \"cosine\" files for Train and Test sets (TrainDataCosine.csv and testDFCosineSimilarity.csv)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since the product description file has poorly spaced sentences (ie. Run-on sentences that represent\n",
    "#the attribute bullet points)\n",
    "#We are going to find the first word of each bullet for each product and apply a space in front of that word\n",
    "#in the product description page\n",
    "bulletPoints = attributes[attributes.name.str.contains('Bullet')== True]\n",
    "\n",
    "#for each row of product descriptions, add in the spaces with the run on sentences\n",
    "prodDesc.product_description = prodDesc.apply(addSpaces, axis = 1)\n",
    "        \n",
    "#Drop every other attribute except the Brand Name\n",
    "attributes = attributes[attributes.name.str.contains('MFG Brand Name')==True]\n",
    "\n",
    "#Combine the remaining attribute categories (attribute names are combined with attribute details, to preserve\n",
    "# knowledge of the types of attributes we've decided to keep\n",
    "#attributes['combined'] = attributes.name.str.cat(attributes.value, sep = ' ')\n",
    "\n",
    "#convert the 'combined' attribute descriptions into String type\n",
    "attributes['name'] = attributes['name'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge our Product Descriptions DF with the Attributes DF, based on common Product IDs\n",
    "mergedProdDesc = prodDesc.merge(attributes, how = 'left', on = 'product_uid')\n",
    "#fill in the NaNs for the products that didn't have a brand name in their attributes section\n",
    "mergedProdDesc.value.fillna('', inplace = True)\n",
    "\n",
    "# \"Product Descriptions\" are combined with their \"attributes\" into one single column of text\n",
    "#this continues to grow our 'bag of words' for each product\n",
    "mergedProdDesc['complete_description'] = mergedProdDesc.product_description.str.cat(mergedProdDesc.value.str.lstrip(), sep = ' ')\n",
    "\n",
    "#get rid of the individual columns now that our description details are combined\n",
    "#mergedProdDesc.drop(['product_description','attributes'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From one of the Kaggle forums, I learned about a generic spell correction algorithm posted by Peter Norvig on http://norvig.com/spell-correct.html <br>\n",
    "It uses a probability based approach of finding the \"correct\" version of an input word from of a large set of \"corrected\" words that we create having an **edit distance** of 1 and 2 from the original input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Source:http://norvig.com/spell-correct.html\n",
    "def words(text): \n",
    "    returnSet = []\n",
    "    for item in text:\n",
    "        returnSet += re.findall('[a-z]+', item.lower())\n",
    "    return returnSet\n",
    "\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "\n",
    "#NWORDS = train(words())\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes    = [a + b[1:] for a, b in splits if b]\n",
    "    transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "    replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "    inserts   = [a + c + b     for a, b in splits for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=NWORDS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word dictionary\n",
      "Cleaning word dict took: 0.14110626777013144 minutes\n"
     ]
    }
   ],
   "source": [
    "# The spell-check algorithm above relies on a \"universe of words\" that we use to compare our potential corrections\n",
    "startTime = time.time()\n",
    "print \"Creating word dictionary\"\n",
    "NWORDS = train(words(mergedProdDesc.complete_description))\n",
    "print \"Cleaning word dict took: %r minutes\" % ((time.time()-startTime)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume the product descriptions and attributes provided to us are spelled correctly. Therefore, we only correct the customer's search terms. <br>\n",
    "Next, we use a 7-step \"cleanText\" process that will help create more uniform text which is more conducive for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spell Checking\n",
      "Cleaning text\n",
      "Cleaning text took: 51.0881606499354 minutes\n"
     ]
    }
   ],
   "source": [
    "#clean all the text that you've combined for each product\n",
    "startTime = time.time()\n",
    "print \"Spell Checking\"\n",
    "trainData.search_term = trainData.search_term.apply(lambda x: spellCheck(x))\n",
    "testData.search_term = testData.search_term.apply(lambda x: spellCheck(x))\n",
    "print \"Cleaning text\"\n",
    "mergedProdDesc.complete_description = mergedProdDesc.complete_description.apply(lambda x: cleanText(x))\n",
    "trainData.search_term = trainData.search_term.apply(lambda x: cleanText(x))\n",
    "testData.search_term = testData.search_term.apply(lambda x: cleanText(x))\n",
    "print \"Cleaning text took: %r minutes\" % ((time.time()-startTime)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Take the product descriptions and attach them to each of the Test data elements\n",
    "cleanTrain = trainData.merge(mergedProdDesc, how = 'left', on = 'product_uid' )\n",
    "#Take the product descriptions and attach them to each of the Test data elements\n",
    "cleanTest = testData.merge(mergedProdDesc, how = 'left', on = 'product_uid' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up and amalgamated all of our relevant product descriptions, we are going to create our **Bag of words** model using **TF-IDF (Term Frequency - Inverse Document Frequency)**. TF-IDF creates a matrix of all words that exist in the \"universe of documents\" you provide, and weights the word counts in such a way as to give greater weight to words that have a higher frequency, but appear in fewer documents. (The logic behind this being that if a word appears in most, if not all, of the documents, then we don't gain very much knowledge from it.) <br>\n",
    "We will use n-grams, with n= 1 to 3. **n-grams** creates combinations of every \"n\" consecutive words, and treats them as 'one word' for the sake of creating the bag-of-words. This way we gain more knowledge through context, instead of just looking at the individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorizer took: 4.367673599720002 minutes\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer( ngram_range =(1,3))\n",
    "\n",
    "startTime = time.time()\n",
    "docMatrix = vectorizer.fit_transform(mergedProdDesc.complete_description)\n",
    "\n",
    "searchTrain = vectorizer.transform(cleanTrain.search_term)\n",
    "descriptionTrain = vectorizer.transform(cleanTrain.complete_description)\n",
    "\n",
    "searchTest = vectorizer.transform(cleanTest.search_term)\n",
    "descriptionTest = vectorizer.transform(cleanTest.complete_description)\n",
    "\n",
    "print \"TF-IDF vectorizer took: %r minutes\" % ((time.time()-startTime)/60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will no doubt have a very large dictionary of words in our bag-of-words matrix, we now have to consider the possibility of high-dimensionality in our data. <br>\n",
    "From a previous Kaggle winner of a natural language processing competition, I learned they had used Singular Vector Decomposition (SVD) in order to reduce dimensionality. A very high level intuition behind this concept is to take a rank reduced approximation of an original dataset matrix, while attempting to maintain the general properties and structure (keep just enough data such that you can still maintain the identity of what you originally started with)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trunc SVD took: 59.09447696606318 minutes\n"
     ]
    }
   ],
   "source": [
    "#Use Singular vector decomposition to reduce the dimensionality of the terms we've collected from the complete\n",
    "#product descriptions\n",
    "\n",
    "startTime = time.time()\n",
    "SVD = TruncatedSVD(n_components = 100)\n",
    "\n",
    "docMatrix = SVD.fit_transform(docMatrix)\n",
    "searchTrain = SVD.transform(searchTrain)\n",
    "descriptionTrain = SVD.transform(descriptionTrain)\n",
    "\n",
    "searchTest = SVD.transform(searchTest)\n",
    "descriptionTest = SVD.transform(descriptionTest)\n",
    "\n",
    "print \"Trunc SVD took: %r minutes\" % ((time.time()-startTime)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first feature we will be adding to our Train and Test dataframes is the **Cosine Similarity**. Cosine Similarity measures the similarity between two vectors of data (in our case, text), ranging from -1 (opposite direction) to +1 (same direction). SKLearn has conveniently provided us with an implementation of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate how \"related\" the search term is to the complete production decription using Cosine Similarity\n",
    "# for TRAINING DATA\n",
    "cosineMatrix = []\n",
    "for myElement in xrange(searchTrain.shape[0]):\n",
    "    cosineMatrix.append(float(cosine_similarity(searchTrain[myElement,:].reshape(1,-1), \n",
    "                                                descriptionTrain[myElement,:].reshape(1,-1))))\n",
    "cleanTrain['cosineSimilarity'] = cosineMatrix\n",
    "cleanTrain.to_csv('data/cleanTrain.csv', index = False)\n",
    "\n",
    "\n",
    "# calculate how \"related\" the search term is to the complete production decription using Cosine Similarity\n",
    "# for TEST DATA\n",
    "cosineMatrix = []\n",
    "for myElement in xrange(searchTest.shape[0]):\n",
    "    cosineMatrix.append(float(cosine_similarity(searchTest[myElement,:].reshape(1,-1), \n",
    "                                                descriptionTest[myElement,:].reshape(1,-1))))\n",
    "\n",
    "# create a new column in the Test Dataframe that records the Cosine Similarity score for each searchTerm vs. \n",
    "# ProdDescrition     \n",
    "cleanTest['cosineSimilarity'] = cosineMatrix\n",
    "\n",
    "#save down the work into a new file so that we can simply import CSV file for future work instead of re-running all\n",
    "cleanTest.to_csv('data/cleanTest.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are only three values that can be assigned to the 'relevance' of search-terms/product combinations, we don't know exactly how many human evaluators are being used and therefore don't know exactly how many outcomes we're predicting. We naturally think about a regression based process, given the seemingly open-endedness of our task. <br>\n",
    "**Linear regression** - very simple and fairly quick model that can allow us to scale up our complexity as desired and allowing us to make a broad range of predictions. The drawback, however, is that we may predict a value above or below our limits. <br>\n",
    "\n",
    "We also consider: <br>\n",
    "**Random Forest** - Allows us to utilize the multiple boundary decision-making process of a decision tree but reduce variance. Variance reduction is done through using many DTs (Ensemble method), each tree using a random subset of training points with replacement (Bagging method), and each tree uses randomly selected features upon which to base our decision boundaries on. \n",
    "\n",
    "**Extremely Randomized Trees (Extra Trees)** - Same as a Random Forest classifier, except that for each feature used in  the decision boundaries, the split value is randomly chosen from that features empirical range.\n",
    "\n",
    "**Gradient Boosting**. - Going one step further, Gradient boosting combines a series of weak learners (for example, the various tree based learners described above), and with each iterative step adds another learner to correct the errors generated from the prior learners. \n",
    "\n",
    "While most of the models we'll implement are part of the SKLearn package, we will also use the **XGBoost** package. XGBoost (Extreme Gradient Boosting) is an optimized, efficient and very accurate implementation of the Gradient Boosting model that has gained notoriety as being a model of choice in many winning Kaggle competitions. \n",
    "\n",
    "Our objective was to minimize the **Root Mean Squared Error**: <br>\n",
    "$$\\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the point from which I begin work again after doing the computation intensive work above. <br>\n",
    "Let's create two more features that might be useful in gaining knowledge. How much of a product's title is present in the search terms and vice versa? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankitpatel/anaconda/lib/python2.7/site-packages/nltk/stem/porter.py:274: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  if word[-1] == 's':\n"
     ]
    }
   ],
   "source": [
    "#************************************\n",
    "#Continue from here: After you've read in or created the cleanTrain/cleanTest dataframe\n",
    "#************************************\n",
    "\n",
    "#scrub the product title\n",
    "cleanTrain.product_title = cleanTrain.product_title.apply(lambda x: cleanText(x))\n",
    "\n",
    "#Calculate what proportion of the given element is present in the other \n",
    "# ie. how many search words are present in product title and how many prod title words are in search term\n",
    "cleanTrain['prodWordMatch'], cleanTrain['searchWordMatch'] = zip(*cleanTrain.apply(compareWordHits, axis = 1))\n",
    "\n",
    "#do the same two-step process for the training set as well\n",
    "cleanTest.product_title = cleanTest.product_title.apply(lambda x: cleanText(x))\n",
    "cleanTest['prodWordMatch'], cleanTest['searchWordMatch'] = zip(*cleanTest.apply(compareWordHits, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying out a few of the models we're considering and see how they fair, using default parameter settings. <br>\n",
    "We can see that the ExtraTrees regressor was the poorest performing model, while XGBoost was the best performing one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False):, score: 0.501030505182299\n",
      "model XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=1, reg_lambda=4,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1):, score: 0.49206259309577255\n",
      "model RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=1, oob_score=False, random_state=0,\n",
      "           verbose=0, warm_start=False):, score: 0.5452820483919778\n",
      "model ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=100, n_jobs=1, oob_score=False, random_state=0,\n",
      "          verbose=0, warm_start=False):, score: 0.5995222148011995\n",
      "model GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.1, loss='ls',\n",
      "             max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "             presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
      "             warm_start=False):, score: 0.49245170475390754\n",
      "model AdaBoostRegressor(base_estimator=None, learning_rate=0.1, loss='linear',\n",
      "         n_estimators=100, random_state=0):, score: 0.500274780132929\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "xData = cleanTrain[['cosineSimilarity','prodWordMatch','searchWordMatch']].values\n",
    "yData = cleanTrain['relevance'].values\n",
    "xTest = cleanTest[['cosineSimilarity','prodWordMatch','searchWordMatch']].values\n",
    "###########################\n",
    "\n",
    "models =[LinearRegression(),\n",
    "         XGBRegressor(max_depth = 5, n_estimators = 100, reg_alpha = 1, reg_lambda = 4),\n",
    "         RandomForestRegressor(n_estimators=100, random_state = 0),\n",
    "         ExtraTreesRegressor(n_estimators=100, random_state = 0),\n",
    "         GradientBoostingRegressor(learning_rate=0.1, n_estimators=200, random_state = 0),\n",
    "         AdaBoostRegressor(n_estimators = 100, random_state = 0, learning_rate = 0.1)]\n",
    "\n",
    "for model in models:\n",
    "    print \"model %s:, score: %r\" %(model, cv_score(model,xData, yData ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stay with the XGBoost model for the moment. We will use the GridSearch function from SKLearn to test a number of different parameter values to find the best combination that optimizes our scoring function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: -0.48\n",
      "[mean: -0.49278, std: 0.00737, params: {'n_estimators': 50, 'max_depth': 4}, mean: -0.49248, std: 0.00864, params: {'n_estimators': 100, 'max_depth': 4}, mean: -0.49239, std: 0.00864, params: {'n_estimators': 200, 'max_depth': 4}, mean: -0.49249, std: 0.00747, params: {'n_estimators': 50, 'max_depth': 5}, mean: -0.49224, std: 0.00881, params: {'n_estimators': 100, 'max_depth': 5}, mean: -0.49251, std: 0.00880, params: {'n_estimators': 200, 'max_depth': 5}, mean: -0.49263, std: 0.00762, params: {'n_estimators': 50, 'max_depth': 6}, mean: -0.49260, std: 0.00892, params: {'n_estimators': 100, 'max_depth': 6}, mean: -0.49318, std: 0.00873, params: {'n_estimators': 200, 'max_depth': 6}] {'n_estimators': 100, 'max_depth': 5} -0.492243407826\n",
      "\n",
      "Time GridSearchCV took to run: 1.6327219168345133 minutes\n"
     ]
    }
   ],
   "source": [
    "# params to be fine tuned on XGB\n",
    "\n",
    "# set up a basic XGB model, with some params that i've already tested before\n",
    "XGB = XGBRegressor(learning_rate = .1)\n",
    "\n",
    "params = {'max_depth': [4,5,6], 'n_estimators': [50,100,200]}\n",
    "\n",
    "xData = cleanTrain[['cosineSimilarity','prodWordMatch','searchWordMatch']].values\n",
    "yData = cleanTrain['relevance'].values\n",
    "XGBgridSearch, gridScores, bestParams, bestScore = do_GridsearchCV(XGB, params, RootMSE, xData, yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'max_depth': 5}\n",
      "-0.492243407826\n"
     ]
    }
   ],
   "source": [
    "# print out what my best parameter value combination is as well as the resulting score\n",
    "print bestParams\n",
    "print bestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.960649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1.907297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2.383601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2.377937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2.229343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  relevance\n",
       "0   1   1.960649\n",
       "1   4   1.907297\n",
       "2   5   2.383601\n",
       "3   6   2.377937\n",
       "4   7   2.229343"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the best parameters found, re-train the XGB model and predict outcomes on test set\n",
    "XGB = XGBRegressor(max_depth = 5, n_estimators = 100, learning_rate = .1)\n",
    "\n",
    "xData = cleanTrain[['cosineSimilarity','prodWordMatch','searchWordMatch']].values\n",
    "yData = cleanTrain['relevance'].values\n",
    "xTest = cleanTest[['cosineSimilarity','prodWordMatch','searchWordMatch']].values\n",
    "XGB.fit(xData, yData)\n",
    "\n",
    "yPred = XGB.predict(xTest)\n",
    "\n",
    "# Assign our predictions to the sample submission format\n",
    "sampleSubmission.relevance = yPred\n",
    "\n",
    "# make sure nothing looks weird\n",
    "sampleSubmission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write the predictions to a csv file that can be uploaded to Kaggle\n",
    "# file name has current data and time appended to it, so different predictions can be distinguished \n",
    "\n",
    "writePath = 'data/XGB_prob_submission_%s.csv' % (datetime.datetime.now())\n",
    "#sampleSubmission.to_csv(writePath, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the methods commonly seen in Kaggle competitions is the use of Ensembling and/or Stacking. <br>\n",
    "Ensembling tends to describe the process of training multiple models, who's individual predictions are weighted in some fashion, subsequently producing one collective prediction. <br>\n",
    "Stacking, on the other hand, trains multiple models, makes predictions using those models, and then trains *another* model on those predictions, ultimately using this combiner model to make a final set of predictions. <br>\n",
    "The intuition behind using these approaches is that we can 'learn' from the different 'opinions' provided by a group of models, which may not be very strong individually, but collectively can be more effective in choosing an accurate prediction (kind of a \"people's choice\", without significant group think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for blending.\n",
      "0 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "1 XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "2 RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=1, oob_score=False, random_state=0,\n",
      "           verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "3 ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=100, n_jobs=1, oob_score=False, random_state=0,\n",
      "          verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "4 GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.1, loss='ls',\n",
      "             max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "             presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "5 AdaBoostRegressor(base_estimator=None, learning_rate=0.1, loss='linear',\n",
      "         n_estimators=100, random_state=0)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Blending.\n",
      "***Score***  0.491729526605\n",
      "Saving to file.\n",
      "Total time taken: 8.373809766769408 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankitpatel/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:516: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=10.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "#Try a model blending approach\n",
    "#Much of this code sourced from: https://github.com/emanuele/kaggle_pbr/blob/master/blend.py\n",
    "\n",
    "###########################\n",
    "X = cleanTrain[['cosineSimilarity','prodWordMatch','searchWordMatch']].values\n",
    "y = cleanTrain['relevance'].values\n",
    "X_submission = cleanTest[['cosineSimilarity','prodWordMatch','searchWordMatch']].values\n",
    "###########################\n",
    "\n",
    "n_folds = 10  #use 10 folds in our cross-validation model building below\n",
    "\n",
    "skf = list(StratifiedKFold(y, n_folds)) # stratifiedKFold provides train/test sets for n_fold iterations, \n",
    "                                        # while maintaining the class proportions of the original target set\n",
    "\n",
    "# our set of base learners (models)\n",
    "clfs =[LinearRegression(),\n",
    "         XGBRegressor(max_depth = 5, n_estimators = 100, learning_rate = 0.1),\n",
    "         RandomForestRegressor(n_estimators=100, random_state = 0),\n",
    "         ExtraTreesRegressor(n_estimators=100, random_state = 0),\n",
    "         GradientBoostingRegressor(learning_rate=0.1, n_estimators=200, random_state = 0),\n",
    "         AdaBoostRegressor(n_estimators = 100, random_state = 0, learning_rate = 0.1)]\n",
    "\n",
    "\n",
    "print \"Creating train and test sets for blending.\"\n",
    "start_time = time.time()\n",
    "#######################################\n",
    "# dataset_blend_train - matrix of (# training samples, # of clfs). It collects all of the predictions made on  \n",
    "#                       'held out' chunks of test sets during the CV process, for each model in our list\n",
    "# dataset_blend_test - matrix of (# of test samples, # of clfs). Collects the average of 10 folds of predictions\n",
    "#                      made on the actual test data; for each model in our list.\n",
    "#######################################\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))\n",
    "    \n",
    "for j, clf in enumerate(clfs):\n",
    "    print j, clf  #print classifier number and model info\n",
    "    dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))  #matrix to hold our predictions on the actual\n",
    "                                                                        #final test set for each fold 'i'\n",
    "    #for each fold 'i', do work with the train/test set created\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print \"Fold\", i\n",
    "        X_train = X[train]\n",
    "        y_train = y[train]\n",
    "        X_test = X[test]\n",
    "        y_test = y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict(X_test)\n",
    "        dataset_blend_train[test, j] = y_submission  #predictions made on our skf test set are added to the train matrix\n",
    "                                                     #in their respective test rows assigned by skf\n",
    "        dataset_blend_test_j[:, i] = clf.predict(X_submission) #predictions made on our actual test data\n",
    "                                                                          #assigned to matrix for each fold  \n",
    "    dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)  #in the overall test matrix, assign the average\n",
    "                                                            #prediction of all i folds on the actual test data\n",
    "                                                            #for each classifier 'j' in our list\n",
    "\n",
    "    # The predictions of the original training set are your new train set\n",
    "    # The average of all predictions of the original test set are your new input test set\n",
    "    \n",
    "print \"Blending.\"\n",
    "\n",
    "# our final \"combiner\" model \n",
    "clf = LinearRegression()\n",
    "\n",
    "# \"train on predictions, and make predictions on predictions\"\n",
    "clf.fit(dataset_blend_train, y)  #train combiner model on the predictions made on the training data by each base model\n",
    "y_submission = clf.predict(dataset_blend_test) #predict outcome given the averaged test predictions of each\n",
    "                                                          #base model\n",
    "\n",
    "#do an independent, local cross-validation to see what the final results might score \n",
    "score = cv_score(clf,dataset_blend_train, y )\n",
    "print \"***Score*** \", score\n",
    "\n",
    "# if i've beaten my benchmark score, then I'm going to save my current results down to csv for Kaggle submission\n",
    "benchmark_score = 0.50\n",
    "if score < benchmark_score:\n",
    "    sampleSubmission.relevance = y_submission\n",
    "    print \"Saving to file.\"\n",
    "    writePath = 'submissions/Blended_submission_%s.csv' % (datetime.datetime.now())\n",
    "    sampleSubmission.to_csv(writePath, index = False)\n",
    "\n",
    "    \n",
    "print \"Total time taken: %r minutes\" % ((time.time()-start_time)/60) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a quick look at the distribution of the final set of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x13169f710>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEZCAYAAACq1zMoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCMSYCIiohEJMBER0Yj/D0vZlghpOPQhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x130556b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_submission, align = 'mid')\n",
    "plt.title(\"Distribution of Final Predictions\")\n",
    "plt.xlabel(\"Classification\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our basic modeling to the more complex stacked model, we didn't really see a significant gain in performance. We can see that our predictions don't have a similar distribution as the training data. Our predictions are rather clustered in the mid 2 range, whereas the training data seemed to be clustered more around the individual classes (1, 2, and 3). This doesn't bode too well for our modeling and is reflected in our local cv score. <br>\n",
    "To improve our results, we probably would need to improve/increase our feature space quite a bit. We only created/used three features, while reading through the Kaggle forums other folks seemed to have many more. <br>\n",
    "Another common issue that was spoken of in the forums, was the correction of spelling mistakes in the search terms. There was a dictionary of search term corrections that was publicly posted, which was obtained by someone who created a script using Google search calls and utlized the \"did you mean\" spell-checked suggestions. I opted to try to find a more generic algorithm. Maybe the next step would be to check to see if those provided spelling corrections would've indeed made a bigger difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
